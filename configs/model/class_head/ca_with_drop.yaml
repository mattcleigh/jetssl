_target_: mltools.mltools.transformers.ClassAttentionPooling
_partial_: true
num_layers: 1
dim: 128
layer_config:
  num_heads: 4
  ff_mult: 2
  dropout: 0.2
  layerscale_init: null # Its just a single layer no need for layer scale
do_input_linear: True
do_output_linear: True
