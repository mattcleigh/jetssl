_target_: src.models.jepa.JetJEPA

ema_start: 0.996
ema_steps: 1000_000

encoder_config:
  do_absolute_enc: True # Trying out JEPA with positional enc
  max_seq_len: 128 # Trying out JEPA with positional enc
  do_final_norm: True # JEPA models all use final norm

predictor_config:
  dim: 256 # JEPA paper used 384
  num_layers: 4  # JEPA paper used 12
  num_registers: 0 # No registers. Already added before predictor.
  do_packed: True # Much faster training but requires half precision
  do_final_norm: True # JEPA models all use final norm... Maybe I should too?
  layer_config:
    num_heads: 4
    ff_mult: 2

defaults:
  - encoder_config: small.yaml
  - optimizer: default.yaml
  - scheduler: cosine.yaml
  - class_head: linear.yaml
  - _self_
