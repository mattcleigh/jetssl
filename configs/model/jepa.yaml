_target_: src.models.jepa.JetJEPA

t_ema: 0.992

encoder_config:
  do_final_norm: False # JEPA models all use final norm

predictor_config:
  dim: 256 # JEPA paper used 384
  num_layers: 4  # JEPA paper used 12
  num_registers: 0 # No registers. Already added before predictor.
  do_packed: True # Much faster training but requires half precision
  do_final_norm: False # JEPA models all use final norm... Maybe I should too?
  layer_config:
    num_heads: 4
    ff_mult: 2

defaults:
  - encoder_config: small.yaml
  - optimizer: default.yaml
  - scheduler: cosine.yaml
  - class_head: ca.yaml
  - _self_
