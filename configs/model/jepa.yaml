_target_: src.models.jepa.JetJEPA

ema_start: 0.992
do_dino: False
do_varcov: False
do_repulse: True
do_cls_loss: False
use_ctxt: False # Bad idea, as the model learns to only use context, boom zero loss
dino_dim: 4096

optimizer:
  # lr: 1.0e-3
  lr: 5.0e-4 # Slower learning rate due to stability issues

encoder_config:
  do_final_norm: True # JEPA models all use final norm
  layer_config:
    layerscale_init: null # Layer-scale can hurt joint embedders

decoder_config:
  do_final_norm: True
  layer_config:
    layerscale_init: null

defaults:
  - encoder_config: small.yaml
  - decoder_config: small.yaml
  - ctxt_config: default.yaml
  - optimizer: default.yaml
  - scheduler: cosine.yaml
  - class_head: ca.yaml
  - _self_
