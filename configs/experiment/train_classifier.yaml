# @package _global_

defaults:
  - override /datamodule: jetclass.yaml
  - override /model: jetclassifier.yaml
  - override /callbacks: finetune.yaml

# Dont train for too long
trainer:
  max_steps: 50000
  # max_epochs: 10

# Key parameters for fine-tuning
# n_jets: 10000 # One tenth of the training set

# Never finetune on the full dataset, always 1 file and n_jets
datamodule:
  train_set:
    n_files: 1
  val_set:
    n_files: 1
  test_set:
    n_files: 1
  loader_config:
    batch_size: 500 # Smaller because some setups have 1000 jets
    num_workers: 1 # Tiny datasets will break with more workers

# For faster training
precision: medium
compile: null

# Bookkeeping
project_name: mpm
network_name: finetuned
