backbone_finetune:
  _target_: src.models.finetuning.CatchupToLR
  unfreeze_at_step: 1000 # One epoch with 1024 batch size
  catchup_steps: 1000

early_stopping:
  _target_: pytorch_lightning.callbacks.EarlyStopping
  monitor: valid/total_loss
  patience: 10
  mode: min

model_checkpoint:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: ${paths.full_path}/checkpoints
  filename: best_{epoch:03d}
  monitor: valid/total_loss
  mode: min
  save_last: True
  auto_insert_metric_name: False

model_save_weights:
  _target_: pytorch_lightning.callbacks.ModelCheckpoint
  dirpath: ${paths.full_path}/saved_weights
  auto_insert_metric_name: False
  save_weights_only: True

model_summary:
  _target_: pytorch_lightning.callbacks.RichModelSummary
  max_depth: 2

lr_monitor:
  _target_: pytorch_lightning.callbacks.LearningRateMonitor
  logging_interval: step
